#!/usr/bin/env python
# used to parse files more easily
from __future__ import with_statement

# Numpy module
import numpy as np

# for command-line arguments
import sys

# Qt4 bindings for core Qt functionalities (non-GUI)
from PyQt4 import QtCore

# Python Qt4 bindings for GUI objects
from PyQt4 import QtGui


# import the MainWindow widget from the converted .ui files
from ui_shaart import Ui_TheMainWindow

import scipy.io.wavfile as wavfile
from scikits.audiolab import Sndfile
import scipy.signal as signal
import pylab as pl
#from matplotlib.backends.backend_qt4agg import NavigationToolbar2QTAgg as NavigationToolbar
from matplotlib.backends.backend_qt4 import NavigationToolbar2QT as NavigationToolbar
import re
import Image
from scipy import zeros, ifft
#import librosa.core as librosa_core

import pyaudio
import time


#----------------- for filtering signals -----------------
# from post by Warren Weckesser, http://tinyurl.com/d4cjs7m
def butter_bandpass(lowcut, highcut, fs, order=5):
    nyq = 0.5 * fs
    low = lowcut / nyq
    if (highcut > nyq):
       highcut = nyq
    high = highcut / nyq
    b, a = signal.butter(order, [low, high], btype='band')
    return b, a

def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):
    b, a = butter_bandpass(lowcut, highcut, fs, order=order)
    y = signal.lfilter(b, a, data)
    return y
#---------------------------------------------------------

# Just slick for debugging:
#def funcname():
#    import inspect
#    return inspect.stack()[1][3]
def funcname():
    import traceback
    return traceback.extract_stack(None, 2)[0][2]


# Global variables
#global amp, orig_amp, t, dB, sample_rate, nofile, filenameA
#global ampB, orig_ampB, tB, dB_B, sample_rateB, nofileB, filenameB
amp = [1.0]
orig_amp = amp
t= [1.0]
dB = [1.0]
sample_rate = 44100
nofile = 1
nofileB = 1
filenameA = ""
ampB = amp
orig_ampB = amp
tB = t
dB_B = dB
sample_rateB = sample_rate
filenameB = filenameA

class DesignerMainWindow(QtGui.QMainWindow, Ui_TheMainWindow):
    """Customization for Qt Designer created window"""
    def __init__(self, parent = None):
        # initialization of the superclass
        super(DesignerMainWindow, self).__init__(parent)
        # setup the GUI --> function generated by pyuic4
        self.setupUi(self)
        # connect the signals with the slots
        QtCore.QObject.connect(self.theactionOpen, QtCore.SIGNAL('triggered()'), self.menuselect_read_fileA)
        QtCore.QObject.connect(self.theactionOpenB, QtCore.SIGNAL('triggered()'), self.menuselect_read_fileB)
        QtCore.QObject.connect(self.theactionQuit, QtCore.SIGNAL('triggered()'), QtGui.qApp, QtCore.SLOT("quit()"))
        QtCore.QObject.connect(self.theactionAbout, QtCore.SIGNAL('triggered()'), self.about_message )
        QtCore.QObject.connect(self.theactionSave, QtCore.SIGNAL('triggered()'), self.write_wav_file)

        QtCore.QObject.connect(self.waveform_abs_checkBox, QtCore.SIGNAL('stateChanged(int)'), self.update_tab)
        QtCore.QObject.connect(self.waveform_env_checkBox, QtCore.SIGNAL('stateChanged(int)'), self.update_tab)

        QtCore.QObject.connect(self.inwavfileselectButton, QtCore.SIGNAL('clicked()'),  self.menuselect_read_fileA )
 #       QtCore.QObject.connect(self.rt60lineEdit, QtCore.SIGNAL('editingFinished()'),   self.changedtext_read_fileA )
        QtCore.QObject.connect(self.inwavfileBselectButton, QtCore.SIGNAL('clicked()'), self.menuselect_read_fileB )
 #       QtCore.QObject.connect(self.fileBlineEdit, QtCore.SIGNAL('editingFinished()'),  self.changedtext_read_fileB )

        QtCore.QObject.connect(self.spectrocmcomboBox, QtCore.SIGNAL('currentIndexChanged(int)'), self.update_tab)


        QtCore.QObject.connect(self.rt60comboBox, QtCore.SIGNAL('currentIndexChanged(QString)'), self.filter_signal)
        QtCore.QObject.connect(self.tabWidget, QtCore.SIGNAL('currentChanged(int)'), self.update_tab )
        QtCore.QObject.connect(self.speedlineEdit, QtCore.SIGNAL('editingFinished()'), self.calc_modes )
        QtCore.QObject.connect(self.lengthlineEdit, QtCore.SIGNAL('editingFinished()'), self.calc_modes )
        QtCore.QObject.connect(self.widthlineEdit, QtCore.SIGNAL('editingFinished()'), self.calc_modes )
        QtCore.QObject.connect(self.heightlineEdit, QtCore.SIGNAL('editingFinished()'), self.calc_modes )
        QtCore.QObject.connect(self.maxmodelineEdit, QtCore.SIGNAL('editingFinished()'), self.calc_modes )

        QtCore.QObject.connect(self.is_img_pushButton, QtCore.SIGNAL('clicked()'), self.select_read_img_file)
        QtCore.QObject.connect(self.is_wav_pushButton, QtCore.SIGNAL('clicked()'), self.select_write_wav_file)
        QtCore.QObject.connect(self.is_go_pushButton, QtCore.SIGNAL('clicked()'), self.img_to_wav)

        QtCore.QObject.connect(self.eq_go_pushButton, QtCore.SIGNAL('clicked()'), self.equation_go)
        QtCore.QObject.connect(self.convolve_go_pushButton, QtCore.SIGNAL('clicked()'), self.convo_go)

        QtCore.QObject.connect(self.pushButton_playrec_go, QtCore.SIGNAL('clicked()'), self.playrec_go)


    # writes a PCM 16 bit WAV file
    def write_wav_file(self):
        global sample_rate, orig_amp
        filename = QtGui.QFileDialog.getSaveFileName(self,"Filename to Save to","",'WAV File Name (*.wav)')
        if filename != "":
            # write to file
            #--------------
            wavfile.write(filename, sample_rate, amp)
        return

    # generic reader routine for audio files.
    def read_audio_file(self, file_name):
        #print "read_audio_file: reading file [",file_name,"]"
        if file_name=='': return
        f = Sndfile(unicode(file_name), 'r')
        wav_data = np.array(f.read_frames(f.nframes), dtype=np.float64)
        samplerate = f.samplerate
        f.close()
        nsamples = len(wav_data)
        if (len(wav_data.shape) > 1):    # take left channel of stereo track
            wav_data = wav_data[:,0]
        y = 1.0*wav_data
        x = np.arange(nsamples)*1.0/samplerate   # time values
        #print "read_audio_file: finished with file [",file_name,"]"
        return y, x, samplerate

    def changedtext_read_fileA(self):
        global amp, filenameA, t, sample_rate, orig_amp
        filenameA = self.rt60lineEdit.text()
        if filenameA=='': return
#        print "changedtext_read_fileA: filenameA = [",filenameA,"]"
        amp, t, sample_rate = self.read_audio_file(filenameA)
        orig_amp = amp
        global nofile
        nofile = 0
        self.update_tab()

    def menuselect_read_fileA(self):
        """opens a file select dialog"""
#        print "menuselect_read_fileA: opening menu"
        # open the dialog and get the selected file
        file = QtGui.QFileDialog.getOpenFileName()
#        print "menuselect_read_fileA: after file="
        # if a file is selected
        if file:
            # update the lineEdit text with the selected filename
            self.rt60lineEdit.setText(file)
            self.changedtext_read_fileA()
            filenameA = "%s" % file

    def changedtext_read_fileB(self):
        global ampB, filenameB, tB, sample_rateB, orig_ampB
        filenameB = self.fileBlineEdit.text()
        ampB, tB, sample_rateB = self.read_audio_file(filenameB)
        orig_ampB = ampB
        global nofileB
        nofileB = 0
        self.update_tab()

    def menuselect_read_fileB(self):
        """opens a file select dialog"""
        # open the dialog and get the selected file
        fileB = QtGui.QFileDialog.getOpenFileName()
        # if a file is selected
        if fileB:
            # update the lineEdit text with the selected filename
            self.fileBlineEdit.setText(fileB)
            self.changedtext_read_fileB()
            filenameB = "%s" % fileB


    def select_read_img_file(self):
        # open the dialog and get the selected file
        file = QtGui.QFileDialog.getOpenFileName()
        if file:
            # update the lineEdit text with the selected filename
            self.is_imname_lineEdit.setText(file)

    def select_write_wav_file(self):
        # open the dialog and get the selected file
        file = QtGui.QFileDialog.getSaveFileName()
        if file:
            # update the lineEdit text with the selected filename
            self.is_wavname_lineEdit.setText(file)

    #-----------------------------------------------------
    # In response to "Octave" comboBox trigger
    # Filters signal for use with rt60 measurements
    #-----------------------------------------------------
    def filter_signal(self):
        global amp, orig_amp, filenameA
        global ampB, orig_ampB, filenameB
        octave_text = str(self.rt60comboBox.currentText())
        #print "filter_signal: at beginning"
        if (octave_text != 'All'):
           octave_center_freq = (float)(re.sub(r' Hz','',octave_text))
           lowcut  = 1.0*(int)(0.71 * octave_center_freq)
           highcut = 1.0*(int)(1.42 * octave_center_freq)
           #print "filter_signal: starting filter"
           amp = butter_bandpass_filter(orig_amp, lowcut, highcut, 1.0*sample_rate, order=3)
           if (len(orig_ampB) > 1):
              ampB = butter_bandpass_filter(orig_ampB, lowcut, highcut, 1.0*sample_rate, order=3)
           #print "filter_signal: finished filtering"
        else:
           amp = orig_amp
           if (len(orig_ampB) > 1):
              ampB = orig_ampB
        self.rt60.linex = [0]   # erase the old line
        self.rt60.liney = [0]   # erase the old line
        #print "filter_signal: calling update_graph"
        self.rt60.update_graph(amp,t,filenameA,ampB,tB,filenameB)
        #print "filter_signal: back from update_graph"
        #print "filter_signal: finished"
        return



    def calc_modes(self):
        vs_text = str(self.speedlineEdit.text())
        x_text = str(self.lengthlineEdit.text())
        y_text = str(self.widthlineEdit.text())
        z_text = str(self.heightlineEdit.text())
        mm_text = str(self.maxmodelineEdit.text())
        if (vs_text=="") | (x_text=="") | (y_text=="") or (z_text==""): return
        vs = float(vs_text)
        x = float(x_text)
        y = float(y_text)
        z = float(z_text)
        maxmodenum = int(mm_text)
        modes = []
        #maxmodenum = 15
        outstring = "Freq (Hz)  Nx Ny Nz\n---------  -- -- -- \n"
        # The following convoluted loops & if statement are simply to enforce a 
        # particular ordering I am fond of for displaying mode numbers
        for modesum in np.arange(1,maxmodenum+1):
           for mm in np.arange(1,modesum+1):
              for i in np.arange(mm,-1,-1):
                 for j in np.arange(mm,-1,-1):
                    for k in np.arange(0,mm+1):
                       if (i+j+k == modesum) & (i<= mm) & (j<=mm) & (k<=mm) & \
                               ((i==mm)| (j==mm) | (k==mm)):
                          f = vs/2.0*np.sqrt( (1.0*i/x)**2 + (1.0*j/y)**2 + (1.0*k/z)**2)
                          mode = [f, i, j, k]
                          modes.append(mode)
        # or we can destroy that ordering by sorting by frequency
        modes.sort()
        for m in modes: 
            thisline = '%9.1f  %2d %2d %2d\n' %  (m[0], m[1], m[2], m[3])
            outstring = outstring + thisline
        self.modesTextEdit.setPlainText(outstring)
        self.modegraph.update_graph(modes,x,y,z)

    def tojas_isft(self, X, fs, T, hop):
       x = scipy.zeros(T*fs)
       framesamp = X.shape[1]
       hopsamp = int(hop*fs)
       for n,i in enumerate(range(0, len(x)-framesamp, hopsamp)):
           x[i:i+framesamp] += scipy.real(scipy.ifft(X[n]))
       return x


    def my_istft(self, X, fs, T):
        # Inverse Short-Time Fourier Transform, i.e. "Inverse Spectrogram"
        # Props to Steve Tjoa, cf.  http://stackoverflow.com/questions/2459295/stft-and-istft-in-python
        # Added the overlapping frames / buffers to improve image quality - Scott Hawley
        # inputs: 
        #   X = image/ stft
        #   fs = sample rate, in samples/sec
        #   T = duration, in secs
        #
        #  Output array will have this structure:
        #                                 pixel                                      pixel
        #  ||----------|--------------------o------------------|-----------------------o-----------------|-----etc
        #               <------------------hop----------------><----------------------hop---------------->
        #                                    <--------------------hop------------------>
        #    <---buf--->                                        <---buf--->          
        #    <---------------------------- frame-------------------------->
        #                                             <--buf--->                                          <--buf--->
        #                                             <---------------------------- frame-------------------------->
        #...and then the starting and ending buffers will be removed
        
        nhops = X.shape[1]              # each pixel is a "hop"
        hop_duration = T / nhops        # in secs
        samples_per_hop = int(hop_duration * fs) 
        
        ibuf = 128 * fs / 44100     # buffer size,  calibrated for 44.1 kHz
        tbuf = 1.0 * ibuf / fs      # may seem redundant, but readable
        x = zeros((T+2*tbuf)*fs)         
        
        # around each hop, we put a frame, centered on the hop, but wider by buf on each side
        samples_per_frame = samples_per_hop + 2*ibuf
        n = samples_per_frame
        
        for ihop in range(nhops):
            b = np.array(ifft(X[ihop], n = n))    # b is a 'vertical' set of pixels
            framedata = np.imag(b)     # I actually find that imag() gives better image results than real() 
            ibgn = ibuf +  samples_per_hop/2 +  ihop*samples_per_hop - samples_per_frame/2
            iend = ibgn + n-1
            x[ibgn:iend] += framedata[0:n-1]
        
        y = x[ibuf:-ibuf]  # chop off the buffers
        return y
        
    def img_to_wav(self):
        im_filename  = str( self.is_imname_lineEdit.text() )
        wav_filename = str( self.is_wavname_lineEdit.text() )
        dur_str = str( self.is_duration_lineEdit.text() )
        rate_str = str( self.is_rate_lineEdit.text() )
        minf_str = str( self.is_minf_lineEdit.text() )
        maxf_str = str( self.is_maxf_lineEdit.text() )
        
        if (im_filename=="") | (wav_filename=="") | (dur_str=="") | (rate_str=="") |(minf_str=="") | (maxf_str=="") : return

        rate = int( rate_str )
        image_duration = float( dur_str ) 
        minfreq = float(minf_str)
        maxfreq = float(maxf_str)

        pic = Image.open(im_filename).convert("LA")
        pic = pic.resize((512, 512), Image.ANTIALIAS)  # my_istft works 'best' with 512x512...
        image = np.array(pic.getdata())
        image = np.array(image[:,0]).reshape(pic.size[1], pic.size[0])


        # Construct the signal. Put the image into X, transpose & flip, take its inverse stft
        #---------------------
        X = 1.0*np.array(image)       # floating point values
        contrast_power = 1.5          # higher = more contrast, but also introduces more distortion
        X = (X)**contrast_power
        X = X.T  # transpose
        X = np.fliplr(X) # flip 

        mysignal = self.my_istft(X, rate, image_duration)    # my routine
#        mysignal = librosa_core.istft(X.T)


        # normalize &  convert the signal to int
        #-------------------------------------------
        maxval = np.max(mysignal)
        mysignal = np.array([1.0*x / maxval for x in mysignal])
        
        iscale = 32767           # This sets the overall volume, 32727 = full scale

        # In the following line, the dtype='i2' selects 16-bit; without it you get 64 bits & no audio
        data = np.array([int(iscale * x) for x in mysignal], dtype='i2')  

        # write to file
        #--------------
        wavfile.write(wav_filename, rate, data)

        self.is_status_label.setText("Finished!  Try opening the WAV file in the Spectrogram!")
           
        return



    #---------------------------------------------------
    # Code for generating sound based on equation
    #---------------------------------------------------
    def equation_go(self):
        global amp, orig_amp, nofile, filenameA, t, sample_rate
        dur_str = str( self.eqnduration_lineEdit.text() )
        rate_str = str( self.eqnsr_lineEdit.text() )
        eq_str = str( self.equation_lineEdit.text() )

        # basic definitions
        TMAX = float( dur_str ) 
        SR = int( rate_str )
        SRm1 = 1.0/SR
        sample_rate = SR
        NS = int( TMAX * sample_rate )
        print 'TMAX, SR, NS = ',TMAX, SR, NS

        #allocate storage
        amp = np.zeros(NS,dtype=np.float64)

        #'parse' the equation string
        eq_str = re.sub('sin','np.sin',eq_str)
        eq_str = re.sub('cos','np.cos',eq_str)
        eq_str = re.sub('tan','np.tan',eq_str)
        eq_str = re.sub('sqrt','np.sqrt',eq_str)
        eq_str = re.sub('ln','np.log',eq_str)
        eq_str = re.sub('log10','np.log10',eq_str)
        eq_str = re.sub('abs','np.abs',eq_str)
        eq_str = re.sub('exp','np.exp',eq_str)
        eq_str = re.sub('PI','3.14159265358979323846',eq_str)
        

        # now create the sounds
        print 'Starting loop'
        for i in range(NS):
           t = i * SRm1
           newstr = "amp[i]  = " + eq_str 
           #print 'newstr = [',newstr,']'
           exec newstr
           #amp[i]  = 0.8 * np.sin( 20 *2*3.14159265358979323846*TMAX/np.log(20000.0/20) * (np.exp(t/TMAX*np.log(20000.0/20))-1) )

        print 'Finished loop'

        # Global settings for commucating with the rest of the program
        orig_amp = amp
        t = np.arange(0.0,TMAX,SRm1)
        nofile = 0
        filenameA = eq_str
        return


    # My own little autocorrelation routine
    def my_autocorr(self,x):
        meanx = np.mean(x)
        x -= meanx
        result = np.correlate(x, x, mode='full')
    #    result = np.correlate(x,x,mode='full')[len(x)-1:];
        maxval = np.max(result);
        result = result / maxval;
#        return result[result.size/2:]
        return result
    

    #---------------------------------------------------
    #  Convolution
    #---------------------------------------------------
    def convo_go(self):
        global orig_amp, sample_rate, amp, t, filenameA
        global orig_ampB, sample_rateB, ampB, tB, filenameB
        global nofile, nofileB

        if ((1==nofileB) and (False == self.checkBox_autocorr.checkState())):
           print "convo_go: Unable to perform convolution"
           return

        if self.checkBox_timerev.checkState():
           amp3 = amp[::-1]
           amp = amp3


        

        if self.checkBox_autocorr.checkState(): # autocorrelation
            print "Debug: autocorrelation is checked"
            amp3 = self.my_autocorr(amp)
        elif (0 == nofileB):   # normal convolution


           #TODO: make this a button
           #print 'adding +3dB/octave filter...'
           #fftamp = np.fft.rfft(amp,n=16384)
           #fftamp = fftamp * range(len(fftamp))
           #amp = np.fft.irfft(fftamp,n=16384)
           #fftamp = np.fft.rfft(ampB,n=16384)
           #fftamp = fftamp * range(len(fftamp))
           #ampB = np.fft.irfft(fftamp,n=16384)


           print 'starting convolution...'
           amp3 = signal.fftconvolve( amp, ampB, mode="same") 
           print 'finished convolution...'


           maxval_3 = np.max(amp3)
           amp3 *= 0.9999999 / maxval_3

        if self.checkBox_removefirsthalf.checkState():
           amp = amp3[amp3.size/2:]    # cut off first half of array
        else:
           amp = amp3 

        orig_amp = amp
        t = np.arange(0.0,amp.size,1.0)/sample_rate;
        amp3 = 0

        nofileB = 1     # wipe out fileB
        ampB = [1.0]
        tB = [1.0]
        filenameB = ""
        return


    #---------------------------------------------------------------------
    # Tab for playing & recording audio
    #---------------------------------------------------------------------
    def playrec_go(self):
        global amp, sample_rate
        global current_index, maxi
        current_index = 0
        maxi = len(amp)-1

    
        # pull a frame_count samples from array "amp"
        def get_chunk(frame_count):
           global current_index

           if ( current_index < maxi):
               iend = np.min(  [current_index + frame_count-1, maxi ])
               chunk = amp[current_index: maxi] 
               out_data = chunk.astype(np.float32).tostring()
               current_index += frame_count 
           else:
               out_data = []
           return out_data

         
        # define callback
        def callback(in_data, frame_count, time_info, status):
           out_data = get_chunk(frame_count)
           return ( out_data , pyaudio.paContinue)
        


        if nofile: return

        # instantiate PyAudio
        p = pyaudio.PyAudio()

        # open stream using callback
        stream = p.open(
                   format=pyaudio.paFloat32,
                   channels=1,
                   rate=sample_rate,
                   output=True,
                   stream_callback=callback)
        
        #start the stream
        stream.start_stream()
        
        # wait for the stream to finish
        while stream.is_active() and (current_index < len(amp)-1) :
           time.sleep(0.1)
        
        #stop stream
        stream.stop_stream()
        stream.close()
        
        #close PyAudio
        p.terminate()

        return



    #----------------------------------------------------------------
    # Generic routine for updating whichever tab is currently showing
    #----------------------------------------------------------------
    def update_tab(self):
        global orig_amp, sample_rate, amp, t, filenameA
        global orig_ampB, sample_rateB, ampB, tB, filenameB
        tabnum = self.tabWidget.currentIndex()
#        print "in update_tab: filenameA = ",filenameA
        if (0 == tabnum):   #equation
            #  self.equation_go()  only run equation_go when the go button is pushed
            return
        elif (1 == tabnum):  # play/rec
            if nofile: return
        elif (2 == tabnum):   # convolve
            if nofile: return
            # self.convo_make(); only run convo_go when the go button is pushed
        elif (3 == tabnum):   # waveform
            self.waveform.update_graph(amp,t,filenameA,ampB,tB,filenameB,
                   self.waveform_abs_checkBox.checkState(),self.waveform_env_checkBox.checkState())
            if nofile: return
        elif (4 == tabnum):   # rt60
            if nofile: return
            self.rt60.update_graph(amp,t,filenameA,ampB,tB,filenameB)
        elif (5 == tabnum):   # power spectrum
            if nofile: return
            self.pwrspec.update_graph(amp,sample_rate,filenameA,ampB,sample_rateB,filenameB)
        elif (6 == tabnum):  # spectrogram
            if nofile: return
            self.spectro.update_graph(amp,sample_rate,self.spectrocmcomboBox.currentIndex())
        elif (7 == tabnum):   # wateverfall
            if nofile: return
            self.water.update_graph(amp,sample_rate)
        elif (8 == tabnum):  # Room Modes
            self.calc_modes()
        elif (9 == tabnum):  # Sabine Eq
            return
        elif (10 == tabnum):  # invSpectro
            self.img_to_wav()
            return
  
    def about_message(self):
        msg = """
SHAART v.0.4 
http://hedges.belmont.edu/~shawley/SHAART
        
A simple audio analysis suite intended for 
educational purposes, student projects, etc.

Scott H. Hawley, Ph.D.
Belmont University
scott.hawley@belmont.edu
        """
        QtGui.QMessageBox.about(self, "About SHAART", msg.strip())

        return

# create the GUI application
app = QtGui.QApplication(sys.argv)
# instantiate the main window
dmw = DesignerMainWindow()
# show it
dmw.show()
# start the Qt main loop execution, exiting from this script
# with the same return code of Qt application
sys.exit(app.exec_())
